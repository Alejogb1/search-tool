[Music] I hope everyone enjoyed the conference yesterday and is looking forward to the sessions today we've got some some really great content lined up so I'm going to kick off the conference with an update on ml flow the open source machine learning platform project that we started at data base last year and I'm really excited today to launch a whole new component of ml flow the model registry for model management so before I talk about that I want to step back a little bit and explain what's difficult about using machine learning and production what ml flow does to tackle that and then I'll start talking about the new component so we see a lot of organizations starting to use machine learning and get great results with it but then the problem that happens you know everyone we talk to in this space is they realize that machine learning development is a lot harder than traditional software development so it's very difficult to launch and maintain these these machine learning applications so why is it difficult so let's break it down actually by looking at you know the differences between traditional software and machine learning so the first difference is actually the goal of what you're trying to do in traditional software the goal is usually just to meet a functional specification for example if I click this button you create a new file you know if this person has this permission they get to see the file and so on so it's pretty clear-cut what the goal is either you meet it or you don't and it's relatively easy to check whether the software meets the goal in contrast in machine learning the goal usually is optimize a metric for example the accuracy of your predictions and heavy improvement in this metric you know heavy 0.1% improvement can translate into you know significant business value and evie degradation and it can be the difference you know between your part you know product working or losing money and as a result because you're trying to optimize this metric you're you want to constantly experiment to improve it you're not done you don't just say okay it works now the second difference is what affects the quality so in traditional software the quality only depends on the code you're hoarding which nice in machine learning but by definition machine learning has programs that generalize from data so the quality depends on the data you put in and also on tuning parameters that you may have to change based on the data so it's much harder to keep track of that and finally in traditional software usually just pick one software stack you know you pick one database technology one web server and so on maybe one programming language and you write your application and in contrast in machine learning because you're trying to you know you're always trying the best algorithms the newest algorithms to improve this business metric you want to be able to compare and combine many libraries models and algorithms for the same tasks so getting your application to make these easy to swap in and out is also challenging and even once your application starts working it's harder to operate the traditional software you just ship it if you don't touch it it usually keeps working with machine learning you have to keep feeding it with new data and if anything breaks in there you're in trouble and the whole process of running it usually involves many different people using different tools for example the data engineer who gets you the data the machine learning engineer who can work on the model and then the application developer who packages and and deploys the model so what are people doing about this problem is obviously quite a bit more complicated than traditional software so the solution that's come up in a lot of places is to design new software that can manage this machine learning application lifecycle and this software is called machine learning platform and the idea here is to encapsulate the lifecycle behind some api's or some some some data structures that let you do things like versioning your data models continuous integration and deployment QA operations monitoring and so on so some of the best-known ml platforms are at the large web companies for example Google tf-x and Facebook FB Lerner but every company we talk with is developing some kind of ml platform so we started looking at the space last year at data breaks we wanted to provide something like this and we realized that there's no open source machine learning platform and everyone is kind of rebuilding the same thing from scratch so that's why we started ml flow an open source and 2n machine platform and in designing ml flow our philosophy was to make sure it works with whatever infrastructure you know your company already has or wants to use so unlike these very specific company internal ones ml flow is based on simple you know REST API is that work with any machine learning library programming language you know deployment infrastructure and so on and in the first version of the project that we launched last June we added the initial components experiment tracking reproducible projects and model packaging and all of these you know have you know a bunch of improvements and and use since then and you know we were really excited about how much the ml flow community has grown we didn't know what would be the interest for something like this but a little over a year later now we already have 140 contributors on the project just to compare you know the team that that I work with the engineering team at database that runs this is only 10 people so lots of external you know non database contributors and we also up to 800,000 downloads a month and for apache spark in contrast when that project started it took about three years to get to the same number of contributors so we're really excited about that so I'm gonna briefly explain what ml flow currently does and then I'll show how the new component the model registry fits in so today I'm just gonna look at all these three components in turn and see how they simplify ml development the first one is experiment tracking so the idea here is as you're developing your machine learning code you want to be able to track you know what version of your code went in what data what parameters and how it performed what metrics it produced and so on so this is just a simple set of REST API is you can use them any way you want your training code you know on-premise in the cloud whatever and you can log information about the environment to this ml flow tracking server and then that provides a central user interface and a central API to see your experiments compare them and keep track of how your model is doing over time so what that looks like once you log a bunch of results you get this UI you can view each one of the experiments you can view the custom things you logged including things like image you can take notes on it too you know to remember to communicate about what was happening in the experiment and then you can take many many ones and and compare them even programmatically to pick sort of the the best model or to see how it's doing over time so very simple but you know much easier than people sending emails around about how they trained their code the other components are also simple and powerful so ml flow projects is just a way in your gate repository for your code you can specify the environment it needs to hone on and then people can on the same project in a reproducible way locally or in the cloud so very simple way to do that and then ml flow models is a way to package models where the users of the model don't need to know what machine learning library is to write it so basically you can take a model written in any library even your own custom code put this in this model package and then the built-in commands that application engineer can use to deploy it for rest serving batch serving or even for generic evaluation and debugging tools so those are those are the three components and we've got a lot of talks about ml floor at the summit you saw QB yesterday talking about millions of models and there are also a bunch of talks today and we also have a lot of activity and in the project has been going on of course as I said with with over 100 contributors so just in the past six months we released version 1.0 of ml flow which stabilizes everything and then we've been really releasing new versions awfully every two months and we have a bunch of powerful new features like automatic logging so that we'd use the amount of code you have to write for that search API based on data frames and integrations with kubernetes HDFS and Selden I'm just going to highlight one of them because I think it's cool it's further improving the tracking component so this is some code here that trains a model using cars you know just your standard machine learning code and with the initial ml flow tracking API you would have to add a bunch of stuff around it to log all the parameters and metrics and the final model you built in 2ml flow and then share it with other people so you know it's nice you can log all this stuff but it's a bunch of work to do it so what we've been doing and tensorflow and curse and hopefully other machine learning library soon as well is we've been adding this auto logging package where you just have to call one line of code Auto log and we automatically capture a lot of the metrics parameter is that the library already knows about so now it's super easy to add ml flow tracking into your existing code and this is an area where you know we'd love to also work with the community to integrate it into other libraries so it's just one example yeah oh thanks yeah oh hope it saves people a bunch of time okay so so that's kind of what ml flow is hopefully has given you an idea so now what's this new thing that we're launching today model management so at the beginning of the year we actually ran a survey of the ml flow users and asked you know what is the next thing you want to see in the project and this came up clearly as number one simplifying model management so what's the problem so most machine learning libraries you know let's let you save your model as a file but there isn't any good software for sharing and collaborating on these files especially with a team so if you're working alone you know you can just save a bunch of files maybe check them into gate or something and you get something like this you know may you have to name the file somehow to keep track of each version and hopefully it's manageable because you remember what you did and what went into each one but if you're working in a large organization with many models this management becomes a major challenge so you might ask for example where can I find the best version of this model am I am I even earning the right one how was it trained you know these files don't tight or any of that tracking information I talked about about how you built it how can I add documentation for it you know make sure that it's compliant or just tell people what I did and also how can we view models before we launch them so inspired by you know software development tools collaborative software development tools such as github we're launching the ml flow model registry which a repository and a collaborative environment for sharing and working on models so what this gives you is this repository of named version models you can add comments and tags on them and people can easily pull you know the latest version of a model through an API and it also has this built in concept of lifecycle stages so each for each model you can have versions that are staging production you know archive development and so on and it's got API is to easily interface with the registry automatically test the models you know do CI CD and so on so how does it's change the workflow so now as a developer you log your model into the registry works with any type of model as long as you can package it in there and other people can go to this and either manually review it or plug in automated tools that use the API and then downstream users can safely no pull the latest model after it's been reviewed and checked to work and you can also use the API and automated jobs or interests serving to make sure your jobs are actually using you know the most stable model or the testing model or whichever one they want so very simple but but very powerful for making this this work we've been developing the model registry in working closely with database customers for a while actually and today we're posting the first version on github with you know with this core functionality so we have a pull request open and it's going to go out in the next open source release of ml flow but you can check it out on github if you you know if you're adventurous and just want to try it and also for database customers it is available there you know if you talk to us we'll enable it for you and work with you to test it out so that's that's the model registry that's kind of enough talking for me I think for you to see you know what this can actually do we'd like you to see a demo and for that I'm going to invite Cori zumar one of the software engineers at ml flow at data breaks to give you guys a demo [Applause] good morning it's great to be here in Amsterdam this city has a long history of innovation and one of the best examples is its windmills for centuries Amsterdam's windmills have been used to churn grain and pump water and now the rest of the globe is catching on embracing wind farms as a source of clean energy unfortunately wind energy is variable it depends on weather factors like wind speed and wind direction however our demand for electricity is incessant so when winds are low energy needs to come from other sources such as burning coal this is challenging for energy providers in order to meet demands while remaining environmentally conscious they must forecast wind power output hours days and even weeks into the future fortunately you can use machine learning to model the power output of a wind farm based on weather forecasts and in the interest of Environmental Protection and data science my colleagues and I decided to try this out we built a machine learning application that periodically forecasts the power output of the largest wind farm in the Netherlands and uses this data to reduce coal consumption now we're gonna see how this is going this dashboard displays the past power output of the wind farm in addition to projections for the future we see that output has ranged from about a thousand megawatts to 2,500 over the last several days but our projections look a little bit strange apparently this massive wind farm will produce no energy for the next week scrolling down we also see that coal consumption is projected to skyrocket in order to compensate this feels a little strange to me so I think we should investigate moving over to our data bricks jobs UI we see we have our production forecasting job and based on the statuses on the right it appears that several recent iterations have failed let's look at the most recent one to see what happens first this job fetches a weather forecast for the wind farm consisting of three features we have temperature wind direction and wind speed forecasted at several points in time next this notebook fetches a machine learning model from an Amazon s3 buckets in this case the model is called model one final and the bucket is called Cates ml models next the model is loaded in Carris and finally our job runs into problems this model can't evaluate the temperature features so let's take stock of what's going on first we have a production application that's supposed to be helping us save the planet but it's not working because somebody named Cates dropped in a bad model second because this model is stored in an Amazon s3 bucket I have no idea how it was built or trained which is gonna make it pretty hard to debug finally there are several people named Kate within my organization and the only one that I know is on vacation to make matters worse I don't know if cliff Cape collaborated with anybody in order to build this model so I'm not sure who else to ask now let's see how we can use the ml flow model registry to overcome some of these challenges to fix our forecasting pipeline and ultimately save the planet first we need to find Kate's machine learning code maybe she uses similar naming conventions for her s3 buckets and her data breaks notebooks I'm gonna pull up the workspace search and try looking for Kate's ML models as I type I see a bunch of folders corresponding to people named Kate but none of them match exactly and because AI is so popular a lot of them have to do with machine learning Kate's models sounds reasonably close let's see what that contains and it looks like we have more folders and notebooks there's a top-secret project we have a language model and awesome there's a directory called model 1 opening this up unfortunately it looks like there are multiple versions we have versions 1 & 3 2 is totally missing final and of course final new we've all been here before final new sounds pretty recent so let's open that up the first cell of this notebook fetches training data for the wind farm let's add a new cell and see what it looks like it appears this data consists of two features wind direction and wind speed sampled across time but temperature is totally missing this may explain our production failures let's add temperature to the list of features and see if we can fetch the data again great this looks a lot better and it matches our production schema so now what we're gonna do is retrain Kate's model on the new data it appears she used a neural network with a single hidden layer and based on this comments in the last cell kate tried several hidden layer sizes before settling on 50 now unfortunately because Kate's didn't use ml flow to track her training I don't know how each layer size specifically impacted performance and because our training data has changed I'm not sure that 50 is still the best choice so what we're gonna do is try several different hidden layer sizes and we're this time we're going to drop in the ml flow tracking library to easily record all this information we'll use the very simple ml flow Karis Auto log routine to simply capture it and then we'll rerun our training as this runs we can open the ml flow run sidebar where we see a new ml flow run for each iteration containing important parameter information such as the number of hidden units as well as metrics such as validation loss once this completes we can refresh the sidebar and see all of our new runs and then select a metric such as validation loss which will allow us to seamlessly find the best model sorting on validation loss we in fact see that a hidden layer size of 200 is optimal for this particular data and we can open this in the ml flow UI we again see a lot of useful information parameters and metrics and scrolling down we also see all of the models artifacts and files selecting this directory allows us to register the model with the ml flow model registry we now have a couple of options we can either create a brand new registered model or we can add a new version to an existing model let's create a brand new model and we're gonna call it Netherlands power forecast model when we hit register we're given a link to the model registry UI and following it displays a bunch of really important information we have the models version version 1 it's date of creation it's author as well as all of the activities and pending transition requests going on around the model we'll come back to these in a moment additionally this UI links directly to the ml flow source run which takes us back to the ml run view we can then click through and view an exact snapshot of the notebook that was used to produce the model providing a complete lineage for our colleagues to later view and this is much easier and much more reliable than searching the workspace for kate's various notebooks and folders navigating back to the UI we see that we don't currently have a description for our model I'm gonna go ahead and add one I will use some markdown and add an overview of the models input features in use case as well as an architecture description and then hit save now that we have a fully tracked and registered model let's use it to fix our production pipeline first I'll move the model into production selecting the stage button we see several model stages each with a unique meaning and set of controllable permissions for example staging is used for model testing while production is used for models that have passed review and are ready to be deployed within your organization you may want to allow many users to move models into staging while reserving production access rights for a select few experts if you are authorized to transition a model into a particular stage you can do so directly if you are not you can request a transition from another member of your organization since I have authorization I'm going to move our model straight into production what could possibly go wrong great our model is in production and if we scroll down we see we have a new activity indicating that the transition occurred this is useful for future reference and audit logging now we're going to integrate our model into our production job I'll navigate back to the job and click through to its notebook this will allow us to modify its behavior I can get rid of all of this complicated s3 download code and instead I am going to import the ml flow PI func module this defines a very useful function called ml flow - load model we can then give it a simple reference to our registered model specifying the production stage and that's it with two lines of code our production job is now linked to our registered model we can return to the we can return to the job UI click through and start a new run and as this executes we can actively monitor its progress at this point it's worth noting that because we have a stable connection between the production job and the model any future development can occur by updating the production version in the model registry we don't have to change any application code and risk introducing errors it looks like our job completed and scrolling down we have some predictions this time comparing the model output to past data it looks like we roughly follow the trends but it's not perfect so as our work continues we can navigate back to the model registry at any time using the convenient side bar icon in the workspace this displays all of the models throughout your organization including their versions and their stages and if we take a look at the power forecasting model it appears that a second version has been registered I think somebody is trying to help us out opening up this version we see that suan one of our resident machine learning experts has developed a PI torch model that she claims is far better for Sol this problem and to prove it she left us a comparison link to the ml flow UI selecting this link I can view the performance relative to the number of training steps and see that indeed sueanne's model achieves much lower validation loss for this particular problem navigating back to the UI because suan is far better at machine learning than i am i will approve her transition request into production and i'll leave a small comment looks good to me when I hit approve we see a new activity indicating this transition occurred and now all we have to do is run the job again we'll go back to the job and we'll hit run in the ml full model registry will automatically pick up the latest production version of the model and even though sueanne's model is trained in pi torch and our first one is trained in Kerris the exact same code can be used to evaluate it ensuring that again no application code needs to be modified in order to achieve successful updates looking at this comment we see or this cell output we see that sueanne's model was loaded this time and if we scroll down we see that her model does a much better job of matching past trends which means that we can be much more confident in the models projections thanks so much for helping out Tsuen now we're gonna navigate back to our dashboard and we're gonna go ahead and refresh now we should see updated power forecasts for our wind farm indeed we're actually going to be getting some power after all and equally importantly coal consumption is actually expected to fall saving us thousands of tons of burned coal over the next several days so now let's recap we've seen that the ML flow model registry helps you solve three critical problems in production machine learning applications first it prevents you from deploying bad models by introducing tools for model administration and review second it integrates with ml flow tracking to provide a complete picture of every model within organization including source code parameters and metrics and finally the model registry provides centralized activity logs recording the entire collaborative process from model developments through deployments completes with rich model descriptions and comments ensuring that absolutely every detail is captured ultimately the ml flow model registry has helped to fix our forecasting pipeline and protect the environment and I can't wait to see how it will help you in your organization thank you [Applause] all right thanks so much great that was awesome that was really exciting to see so that's their mail flow model registry I hope everyone enjoys using it and if you want to get started using ml flow it's very simple you can pip install ml flow you can find the documentation online and also if you just want to try a version that's hosted we just added ml flow today to the database Community Edition which is a free small scale version of database so you can also quickly log in and try that out thanks and I hope you enjoy using ml flow [Applause]