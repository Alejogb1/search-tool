import sys
import os
import csv
import uuid
from core.services.keyword_enricher import KeywordEnricher

async def run_keyword_workflow(domain: str) -> str:
    """Optimized workflow returning raw CSV of expanded keywords"""
    csv_filename = f"output-{uuid.uuid4()}.csv"
    csv_path = os.path.abspath(csv_filename)
    
    try:
        from integrations.llm_client import generate_with_multiple_keys
        api_keys_str = os.getenv("GOOGLE_API_KEYS")
        API_KEYS = [key.strip() for key in api_keys_str.split(',')]
        MODEL_CONFIGS = [
            {"name": "gemini-2.5-pro", "rpm": 5},
            {"name": "gemini-2.5-flash", "rpm": 10},
            {"name": "gemini-2.5-flash-lite-preview-06-17", "rpm": 15},
            {"name": "gemini-2.0-flash", "rpm": 15},
            {"name": "gemini-2.0-flash-lite", "rpm": 30},
        ]
        

        # Generate seed keywords
        seed_keywords = await generate_with_multiple_keys( context_source="domain",
        domain_url=domain,
        api_keys=API_KEYS, 
        model_configs=MODEL_CONFIGS, 
        output_file='input-keywords-DOMAIN-.txt', 
        parallel=False, 
        mode="commercial")
        if not seed_keywords:
            raise ValueError("No keywords generated by LLM")
            
        # Expand keywords
        # Note: customer_id is a placeholder here and should be managed properly
        enricher = KeywordEnricher(customer_id="dummy_id", output_file=csv_path)
        expanded_keywords = enricher.expand_keywords(seed_keywords)
        
        # Write raw CSV output
        with open(csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['keyword'])
            for kw in expanded_keywords:
                writer.writerow([kw.get('text', '')])
        
        return csv_path
        
    except Exception as e:
        # Clean up the temporary file if an error occurs
        if os.path.exists(csv_path):
            os.remove(csv_path)
        raise
    finally:
        # Ensure the temporary file is removed after successful execution
        if os.path.exists(csv_path):
            os.remove(csv_path)

# The legacy run_full_analysis method is kept for backward compatibility,
# but it's not directly used by the new API endpoints.
# It also contains FastAPI imports which are not relevant here.
# For clarity, it's commented out. If needed, it should be refactored
# to remove FastAPI dependencies or moved to an API-specific module.

# async def run_full_analysis(self, domain_url: str, customer_id: str):
#     """
#     Orchestrates the full analysis pipeline (Legacy):
#     1. Generate seed keywords.
#     2. Enrich keywords with metrics.
#     3. Cluster keywords.
#     4. Build knowledge graph (future step).
#     """
#     print(f"Starting full analysis for domain: {domain_url}")
#     db_domain = None
#     
#     try:
#         # Generate seed keywords using LLM
#         print("Step 1: Generating seed keywords with LLM...")
#         from integrations.llm_client import generate_seed_keywords
#         seed_keywords = generate_seed_keywords(domain_url)
#         
#         if not seed_keywords:
#             print("No seed keywords found in input-keywords.txt. Aborting analysis.")
#             return
#
#         # Step 2: Enrich Keywords (using the refactored service)
#         print("Step 2: Enriching keywords with search metrics...")
#         enricher = KeywordEnricher(customer_id=customer_id, output_file='output-keywords.csv')
#         enriched_keywords = enricher.expand_keywords(seed_keywords)
#         
#         if not enriched_keywords:
#             print("No keywords were enriched. Check API credentials and customer ID.")
#             # Decide if to proceed with only seed keywords
#         
#         # Store the enriched keywords in the database
#         # This logic will replace the old repository call
#         print("Storing keywords and metrics in the database...")
#         db_domain = repository.create_domain_if_not_exists(self.db, domain_url)
#         repository.create_keywords_for_domain(self.db, db_domain.id, enriched_keywords)
#         print(f"{len(enriched_keywords)} enriched keywords stored for domain {db_domain.id}.")
#
#         # Step 3: Cluster Keywords (Future Implementation)
#         print("Step 3: Clustering keywords (not yet implemented)...")
#         # embeddings = self.keyword_clusterer.generate_embeddings(enriched_keywords)
#         # clusters = self.keyword_clusterer.perform_clustering(embeddings)
#         # repository.store_clusters(self.db, clusters)
#
#         # Step 4: Build Knowledge Graph (Future Implementation)
#         print("Step 4: Building knowledge graph (not yet implemented)...")
#
#         print("Analysis pipeline completed successfully.")
#
#     except Exception as e:
#         print(f"An error occurred during the analysis pipeline: {e}")
#         # Optionally, add more robust error handling and rollback logic
#     finally:
#         print("Closing database session.")
#         self.db.close()
#
# async def main():
#     # This is an example of how to run the orchestrator
#     # In a real application, this would be triggered by an API call.
#     db_session = SessionLocal()
#     customer_id = "ID-123131"  # This should be managed securely (e.g., from config)
#     orchestrator = AnalysisOrchestrator(db_session)
#     await orchestrator.run_full_analysis(domain_url="https://www.getgalaxy.io", customer_id=customer_id)
#
# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)
